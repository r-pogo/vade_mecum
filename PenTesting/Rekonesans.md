## WstÄ™p
1. Testy eksploracyjne manualne
2. View page source
3. Developer tools

### Content Discovery  
- `Robots.txt`: The robots.txt file is a document that tells search engines which pages they are and aren't allowed to show on their search engine results or ban specific search engines from crawling the website altogether.  
- `Favicon`: The favicon is a small icon displayed in the browser's address bar or tab used for branding a website, it can point to information about framework used.  
[OWSAP database for favicon](https://wiki.owasp.org/index.php/OWASP_favicon_database)  
The Framework Stack can be used to find the framework website to learn more about the software and other information, possibly leading to more content we can discover.  
- `Sitemap.xml`:Unlike the robots.txt file, which restricts what search engine crawlers can look at, the sitemap.xml file gives a list of every file the website owner wishes to be listed on a search engine.  
- `HTTP Headers`: Headers can sometimes contain useful information such as the webserver software and possibly the programming/scripting language in use.  

### OSINT
`Google Hacking / Dorking`: Example:  

| Filter   | Example              | Description                                              |
|----------|----------------------|----------------------------------------------------------|
| site     | `site:tryhackme.com` | Returns results only from the specified website address |
| inurl    | `inurl:admin`        | Returns results that have the specified word in the URL |
| filetype | `filetype:pdf`       | Returns results with a particular file extension       |
| intitle  | `intitle:admin`      | Returns results that contain the specified word in the title |

- [Wappalyzer](https://www.wappalyzer.com/): is an online tool and browser extension that helps identify what technologies a website uses, such as frameworks, Content Management Systems (CMS), payment processors and much more, and it can even find version numbers as well.  
- [Wayback Machine](https://archive.org/web/): is a historical archive of websites that dates back to the late 90s. You can search a domain name, and it will show you all the times the service scraped the web page and saved the contents. This service can help uncover old pages that may still be active on the current website.  
- `Github`
- `S3 Buckets`: S3 Buckets are a storage service provided by Amazon AWS, allowing people to save files and even static website content in the cloud accessible over HTTP and HTTPS.  
 The format of the S3 buckets is http(s)://{name}.s3.amazonaws.com where {name} is decided by the owner, such as tryhackme-assets.s3.amazonaws.com. S3 buckets can be discovered in many ways, such as finding the URLs in the website's page source, GitHub repositories, or even automating the process. One common automation method is by using the company name followed by common terms such as {name}-assets, {name}-www, {name}-public, {name}-private, etc.  
- `ffuf` for automated discovery.

___
## Sources
- tryhackme, Content Discovery, https://tryhackme.com/room/contentdiscovery.